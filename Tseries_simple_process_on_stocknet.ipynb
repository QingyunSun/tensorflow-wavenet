{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tseries_simple process on stocknet.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/QingyunSun/tensorflow-wavenet/blob/master/Tseries_simple_process_on_stocknet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cy-2lwKUdm1T",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "> Process Data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQdAuGDGkFG2",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "universe = ['AAPL',\n",
        " 'ABB',\n",
        " 'ABBV',\n",
        " 'AEP',\n",
        " 'AGFS',\n",
        " 'AMGN',\n",
        " 'AMZN',\n",
        " 'BA',\n",
        " 'BABA',\n",
        " 'BAC',\n",
        " 'BBL',\n",
        " 'BCH',\n",
        " 'BHP',\n",
        " 'BP',\n",
        " 'BRK-A',\n",
        " 'BSAC',\n",
        " 'BUD',\n",
        " 'C',\n",
        " 'CAT',\n",
        " 'CELG',\n",
        " 'CHL',\n",
        " 'CHTR',\n",
        " 'CMCSA',\n",
        " 'CODI',\n",
        " 'CSCO',\n",
        " 'CVX',\n",
        " 'D',\n",
        " 'DHR',\n",
        " 'DIS',\n",
        " 'DUK',\n",
        " 'EXC',\n",
        " 'FB',\n",
        " 'GD',\n",
        " 'GE',\n",
        " 'GMRE',\n",
        " 'GOOG',\n",
        " 'HD',\n",
        " 'HON',\n",
        " 'HRG',\n",
        " 'HSBC',\n",
        " 'IEP',\n",
        " 'INTC',\n",
        " 'JNJ',\n",
        " 'JPM',\n",
        " 'KO',\n",
        " 'LMT',\n",
        " 'MA',\n",
        " 'MCD',\n",
        " 'MDT',\n",
        " 'MMM',\n",
        " 'MO',\n",
        " 'MRK',\n",
        " 'MSFT',\n",
        " 'NEE',\n",
        " 'NGG',\n",
        " 'NVS',\n",
        " 'ORCL',\n",
        " 'PCG',\n",
        " 'PCLN',\n",
        " 'PEP',\n",
        " 'PFE',\n",
        " 'PG',\n",
        " 'PICO',\n",
        " 'PM',\n",
        " 'PPL',\n",
        " 'PTR',\n",
        " 'RDS-B',\n",
        " 'REX',\n",
        " 'SLB',\n",
        " 'SNP',\n",
        " 'SNY',\n",
        " 'SO',\n",
        " 'SPLP',\n",
        " 'SRE',\n",
        " 'T',\n",
        " 'TM',\n",
        " 'TOT',\n",
        " 'TSM',\n",
        " 'UL',\n",
        " 'UN',\n",
        " 'UNH',\n",
        " 'UPS',\n",
        " 'UTX',\n",
        " 'V',\n",
        " 'VZ',\n",
        " 'WFC',\n",
        " 'WMT',\n",
        " 'XOM']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYvlsKQ6mEyh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime as dt\n",
        "import xarray as xr\n",
        "import sys, os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-7IFXvnl8w5",
        "colab_type": "code",
        "outputId": "d9dbcc85-1bd2-4d2f-ceb7-d8a957f7c774",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3aa9f01a-7156-4bd0-919b-a2ccd570f257\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-3aa9f01a-7156-4bd0-919b-a2ccd570f257\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving AAPL.csv to AAPL.csv\n",
            "Saving ABB.csv to ABB.csv\n",
            "Saving ABBV.csv to ABBV.csv\n",
            "Saving AEP.csv to AEP.csv\n",
            "Saving AGFS.csv to AGFS.csv\n",
            "Saving AMGN.csv to AMGN.csv\n",
            "Saving AMZN.csv to AMZN.csv\n",
            "Saving BA.csv to BA.csv\n",
            "Saving BABA.csv to BABA.csv\n",
            "Saving BAC.csv to BAC.csv\n",
            "Saving BBL.csv to BBL.csv\n",
            "Saving BCH.csv to BCH.csv\n",
            "Saving BHP.csv to BHP.csv\n",
            "Saving BP.csv to BP.csv\n",
            "Saving BRK-A.csv to BRK-A.csv\n",
            "Saving BSAC.csv to BSAC.csv\n",
            "Saving BUD.csv to BUD.csv\n",
            "Saving C.csv to C.csv\n",
            "Saving CAT.csv to CAT.csv\n",
            "Saving CELG.csv to CELG.csv\n",
            "Saving CHL.csv to CHL.csv\n",
            "Saving CHTR.csv to CHTR.csv\n",
            "Saving CMCSA.csv to CMCSA.csv\n",
            "Saving CODI.csv to CODI.csv\n",
            "Saving CSCO.csv to CSCO.csv\n",
            "Saving CVX.csv to CVX.csv\n",
            "Saving D.csv to D.csv\n",
            "Saving DHR.csv to DHR.csv\n",
            "Saving DIS.csv to DIS.csv\n",
            "Saving DUK.csv to DUK.csv\n",
            "Saving EXC.csv to EXC.csv\n",
            "Saving FB.csv to FB.csv\n",
            "Saving GD.csv to GD.csv\n",
            "Saving GE.csv to GE.csv\n",
            "Saving GMRE.csv to GMRE.csv\n",
            "Saving GOOG.csv to GOOG.csv\n",
            "Saving HD.csv to HD.csv\n",
            "Saving HON.csv to HON.csv\n",
            "Saving HRG.csv to HRG.csv\n",
            "Saving HSBC.csv to HSBC.csv\n",
            "Saving IEP.csv to IEP.csv\n",
            "Saving INTC.csv to INTC.csv\n",
            "Saving JNJ.csv to JNJ.csv\n",
            "Saving JPM.csv to JPM.csv\n",
            "Saving KO.csv to KO.csv\n",
            "Saving LMT.csv to LMT.csv\n",
            "Saving MA.csv to MA.csv\n",
            "Saving MCD.csv to MCD.csv\n",
            "Saving MDT.csv to MDT.csv\n",
            "Saving MMM.csv to MMM.csv\n",
            "Saving MO.csv to MO.csv\n",
            "Saving MRK.csv to MRK.csv\n",
            "Saving MSFT.csv to MSFT.csv\n",
            "Saving NEE.csv to NEE.csv\n",
            "Saving NGG.csv to NGG.csv\n",
            "Saving NVS.csv to NVS.csv\n",
            "Saving ORCL.csv to ORCL.csv\n",
            "Saving PCG.csv to PCG.csv\n",
            "Saving PCLN.csv to PCLN.csv\n",
            "Saving PEP.csv to PEP.csv\n",
            "Saving PFE.csv to PFE.csv\n",
            "Saving PG.csv to PG.csv\n",
            "Saving PICO.csv to PICO.csv\n",
            "Saving PM.csv to PM.csv\n",
            "Saving PPL.csv to PPL.csv\n",
            "Saving PTR.csv to PTR.csv\n",
            "Saving RDS-B.csv to RDS-B.csv\n",
            "Saving REX.csv to REX.csv\n",
            "Saving SLB.csv to SLB.csv\n",
            "Saving SNP.csv to SNP.csv\n",
            "Saving SNY.csv to SNY.csv\n",
            "Saving SO.csv to SO.csv\n",
            "Saving SPLP.csv to SPLP.csv\n",
            "Saving SRE.csv to SRE.csv\n",
            "Saving T.csv to T.csv\n",
            "Saving TM.csv to TM.csv\n",
            "Saving TOT.csv to TOT.csv\n",
            "Saving TSM.csv to TSM.csv\n",
            "Saving UL.csv to UL.csv\n",
            "Saving UN.csv to UN.csv\n",
            "Saving UNH.csv to UNH.csv\n",
            "Saving UPS.csv to UPS.csv\n",
            "Saving UTX.csv to UTX.csv\n",
            "Saving V.csv to V.csv\n",
            "Saving VZ.csv to VZ.csv\n",
            "Saving WFC.csv to WFC.csv\n",
            "Saving WMT.csv to WMT.csv\n",
            "Saving XOM.csv to XOM.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Fgvl0pnmuxu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import io\n",
        "from collections import OrderedDict\n",
        "\n",
        "data = OrderedDict()\n",
        "for u in universe:\n",
        "  file = '{u}.csv'.format(u=u)\n",
        "  data[u] = pd.read_csv(io.BytesIO(uploaded[file]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uIZVl6epA0k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_feature(df):\n",
        "    log_adj_close_delta = np.log(df.iloc[1:][\"Adj Close\"] / df.iloc[:-1][\"Adj Close\"].values)\n",
        "    log_volumn_delta = np.log(df.iloc[1:][\"Volume\"] / df.iloc[:-1][\"Volume\"].values)\n",
        "    high_low = (df.iloc[1:][\"High\"] - df.iloc[1:][\"Low\"]) / (df.iloc[1:][\"High\"] + df.iloc[1:][\"Low\"]) * 2.0\n",
        "    open_close = (df.iloc[1:][\"Open\"] - df.iloc[1:][\"Close\"]) / (df.iloc[1:][\"Open\"] + df.iloc[1:][\"Close\"]) * 2.0\n",
        "    open_close_high_low = (df.iloc[1:][\"Open\"] - df.iloc[1:][\"Close\"]) / (df.iloc[1:][\"High\"] - df.iloc[1:][\"Low\"] + 1e-4)\n",
        "    feature = pd.DataFrame(np.stack([log_adj_close_delta,\n",
        "                        log_volumn_delta, high_low,\n",
        "                        open_close, open_close_high_low], axis=-1), index=df.iloc[1:][\"Date\"])\n",
        "\n",
        "    return feature"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ii_zKNmsSWBz",
        "colab_type": "code",
        "outputId": "c3c210be-395f-473e-af34-af62a7e05c29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "N_STOCK = len(universe)\n",
        "total_days = 1258\n",
        "days = data[\"AAPL\"][\"Date\"][1:].values\n",
        "N_FEATURE = 5\n",
        "\n",
        "feature_data = xr.DataArray(np.zeros((total_days-1, N_STOCK, N_FEATURE)), \n",
        "                            dims = (\"time\", \"symbols\", \"features\"),\n",
        "                            coords = {\"time\": days, \"symbols\": universe,\n",
        "                                      \"features\": [\"log_adj_close_delta\", \"log_volumn_delta\", \"high_low\", \"open_close\", \"open_close_high_low\"]})\n",
        "for st in universe:\n",
        "  if data[st].shape[0] != total_days:\n",
        "    print('{s} does not have enough days'.format(s = st))\n",
        "  feature = extract_feature(data[st])\n",
        "  feature_data.loc[feature.index, st, :] = feature.values"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ABBV does not have enough days\n",
            "AGFS does not have enough days\n",
            "BABA does not have enough days\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/series.py:679: RuntimeWarning: divide by zero encountered in log\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "GMRE does not have enough days\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgyYtao-U_h7",
        "colab_type": "code",
        "outputId": "7acf611d-e9c7-4c63-924b-10fd515f4f34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "feature_data = feature_data.fillna(0)\n",
        "feature_data[np.where(np.isinf(feature_data))] = 0.0\n",
        "feature_data[np.where(feature_data > 6)] = 6.0\n",
        "feature_data[np.where(feature_data < -6)] = -6.0\n",
        "feature_data.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1257, 88, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_gUbFT4HhtH",
        "colab_type": "code",
        "outputId": "1ebae85d-6477-469d-ee7b-a657aa580f2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "FR=0\n",
        "returns = feature_data[:, :, 0]\n",
        "for ti in range(100):\n",
        "    daily_FR = np.corrcoef(returns.values[ti, :], returns.values[ti+1, :])[0,1] * np.std(returns.values[ti+1, :])\n",
        "    print(ti, daily_FR, np.corrcoef(returns.values[ti, :], returns.values[ti+1, :])[0,1])\n",
        "    FR = FR + daily_FR\n",
        "print(FR/100*1e4)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0.0006848670660560301 0.0596607160190975\n",
            "1 0.006227054838831885 0.43215446104663197\n",
            "2 0.00045971194211550184 0.04090395345704579\n",
            "3 -0.0019156734239017962 -0.1706107672936251\n",
            "4 0.0038648001895091565 0.33721493206320224\n",
            "5 -0.0015196456744907146 -0.1542682515331423\n",
            "6 0.0019170049617581507 0.12881035988492393\n",
            "7 -0.004721294570210256 -0.4938241353376767\n",
            "8 0.0012038698417875405 0.11671400601582987\n",
            "9 0.0009425428377918865 0.10149431918992582\n",
            "10 -0.0028201507268577307 -0.3177274956592921\n",
            "11 4.413614627436219e-05 0.0048386348515283145\n",
            "12 -0.0006107842400415625 -0.04667250288417307\n",
            "13 0.003016912009301817 0.26205272213565134\n",
            "14 0.0016745836558960225 0.1895709049337784\n",
            "15 -0.005871644453214717 -0.5484816345625287\n",
            "16 -0.002441054858359248 -0.19185883665569192\n",
            "17 -0.00078373387768372 -0.09579265428936576\n",
            "18 -0.0009037434950813448 -0.12847077157509232\n",
            "19 -0.0009666244378869931 -0.10039072179171633\n",
            "20 0.0016418584919365112 0.19006562062409865\n",
            "21 -0.00170760024836933 -0.1896194831167164\n",
            "22 -0.0001398878447010135 -0.016820811778222148\n",
            "23 0.0005709635373129514 0.05807786115533872\n",
            "24 0.0031649311218785244 0.3002458346062841\n",
            "25 0.002863866698491069 0.30007595517860725\n",
            "26 0.00019400849910717032 0.020470907971559982\n",
            "27 -0.0021621500311616243 -0.219253271079357\n",
            "28 0.0024115853245452847 0.25957868789751026\n",
            "29 -0.0005360260701704051 -0.049019133982495536\n",
            "30 0.0020895387986801157 0.13995442010285084\n",
            "31 0.0006532021860168589 0.05346001014906994\n",
            "32 -0.0010740167569931417 -0.09984782208777312\n",
            "33 -0.0011153541490525525 -0.09935079465986521\n",
            "34 0.001072349963990868 0.050587300839553206\n",
            "35 -0.002741214008649816 -0.2704286223163455\n",
            "36 -0.0017496508059352837 -0.14245226642115116\n",
            "37 -0.001262354968847068 -0.11116177037598247\n",
            "38 0.0010983539222749687 0.07587125974734214\n",
            "39 0.0011673965971090083 0.0944335191181264\n",
            "40 -0.000768533836264518 -0.0708007898973619\n",
            "41 0.002641625017761341 0.2735768286210515\n",
            "42 -0.004278746911707559 -0.2889071993875761\n",
            "43 0.0013269731165475931 0.12900501318531035\n",
            "44 -5.3949442578017476e-05 -0.004387623695807087\n",
            "45 -0.0005834784563784802 -0.04643418447319831\n",
            "46 0.0001934548081065926 0.01792879192564448\n",
            "47 -0.001705568002491208 -0.08097670167165835\n",
            "48 -7.57293821567185e-05 -0.006114848314405515\n",
            "49 0.00021496588373183303 0.021654803318828367\n",
            "50 -0.0038482992316478492 -0.28581859608001847\n",
            "51 0.0006277704668512911 0.06387426265911955\n",
            "52 0.0013068942650694777 0.14826200617105303\n",
            "53 0.00014852237673595943 0.015760840257851943\n",
            "54 -0.004429540021654738 -0.3523414254278665\n",
            "55 0.002245505614372122 0.28282063900805543\n",
            "56 3.714258866696962e-05 0.004983069966902298\n",
            "57 -0.0012935492388764863 -0.14610941860575855\n",
            "58 0.0011562496223243405 0.12155664803083345\n",
            "59 -0.0029124429398702193 -0.3737375688771747\n",
            "60 -0.0006596842200295212 -0.08694016294698742\n",
            "61 0.0029290106913646015 0.20343614694049433\n",
            "62 0.00010241157589643534 0.013423708349318319\n",
            "63 -0.0005063411980765558 -0.07190661430199748\n",
            "64 0.00011942554554216324 0.010896111040167035\n",
            "65 0.005792109229481097 0.4781851835520393\n",
            "66 -0.003224749260006494 -0.3774588031854777\n",
            "67 0.014525184703305853 0.43650947166416293\n",
            "68 -0.0006498743751755516 -0.06264070145447194\n",
            "69 -0.0006805295285023478 -0.06992024187763475\n",
            "70 0.003520620558991834 0.32624233491420795\n",
            "71 0.0018774148994083165 0.20330754411854537\n",
            "72 -0.0010724994073972008 -0.12127176535534917\n",
            "73 -0.0015840915532368887 -0.20680885628794587\n",
            "74 -0.0019473308450986043 -0.3154080256189414\n",
            "75 -0.0028541194568257548 -0.3422187471649866\n",
            "76 -0.0005120615669440635 -0.08435490380114583\n",
            "77 0.0008869402349549882 0.11957726357604598\n",
            "78 -0.0006276598125837857 -0.05381091136818415\n",
            "79 0.006272154110354397 0.4792007895313003\n",
            "80 0.0011978427600834837 0.12708326388117594\n",
            "81 -0.0005556672318322327 -0.04921371550560202\n",
            "82 0.0009974576513362768 0.08967037852204575\n",
            "83 0.0038381498954786863 0.31383510498985895\n",
            "84 -0.00027335754936032124 -0.021786965808774365\n",
            "85 -0.0032231869730695974 -0.24718271922291168\n",
            "86 -0.002057814446931637 -0.21433381571030602\n",
            "87 0.0019343384083085494 0.19349570949817482\n",
            "88 0.0034125361121044545 0.38750881489710065\n",
            "89 0.0009560900437068135 0.08734002583448305\n",
            "90 -0.003238503317165408 -0.2916068742119166\n",
            "91 -0.0007094854448156585 -0.055972937748099576\n",
            "92 -0.0007971210077415979 -0.08451109509614592\n",
            "93 -0.0009552511139419123 -0.0852302143582431\n",
            "94 -0.0028891461657242503 -0.16541818279555703\n",
            "95 0.0036118538743122207 0.32240529231018\n",
            "96 -0.002249483597772408 -0.20493830497223656\n",
            "97 0.0011073037323208225 0.08188179117655907\n",
            "98 -0.004132384039148955 -0.4575160425945291\n",
            "99 -0.0022145195883006568 -0.18191328206467192\n",
            "0.733618261286868\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "embGN9AXpBf1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "WINDOW_SIZE = 20\n",
        "\n",
        "def rolling_window_dataset(ti_start, ti_stop, feature_data):\n",
        "  x = np.zeros((ti_stop-ti_start-WINDOW_SIZE, WINDOW_SIZE, N_STOCK, N_FEATURE))\n",
        "  y = np.zeros((ti_stop-ti_start-WINDOW_SIZE, N_STOCK))\n",
        "  for i in range(ti_start, ti_stop-WINDOW_SIZE):\n",
        "    x[i-ti_start, :, :, :] = feature_data[i:i+WINDOW_SIZE, :, :].values\n",
        "    y[i-ti_start, :] = feature_data[i+WINDOW_SIZE, :, :].loc[:, \"log_adj_close_delta\"].values\n",
        "\n",
        "  return x, y\n",
        "\n",
        "x_train, y_train = rolling_window_dataset(0, 760, feature_data)\n",
        "x_val, y_val = rolling_window_dataset(760, 1010, feature_data)\n",
        "x_test, y_test = rolling_window_dataset(1010, feature_data.shape[0]-1, feature_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKtVtLbfdwFW",
        "colab_type": "text"
      },
      "source": [
        "> Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHWn4ejId3f9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow.compat.v2 as tf\n",
        "import tensorflow.compat.v1 as tfv1\n",
        "tf.enable_v2_behavior()\n",
        "tfv1.enable_v2_behavior()\n",
        "from tensorflow.compat.v2 import keras\n",
        "from tensorflow.compat.v2.keras import layers, optimizers, datasets\n",
        "import tensorflow.compat.v2.keras.backend as K\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "import numpy as np\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlmWBTnvubWS",
        "colab_type": "text"
      },
      "source": [
        "Relative Position Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1twXyNcuaqd",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "@tf.function\n",
        "def _generate_relative_positions_matrix(length, max_relative_position):\n",
        "  \"\"\"Generates matrix of relative positions between inputs.\"\"\"\n",
        "  range_vec = tf.range(length)\n",
        "  range_mat = tf.reshape(tf.tile(range_vec, [length]), [length, length])\n",
        "  distance_mat = range_mat - tf.transpose(range_mat)\n",
        "  distance_mat_clipped = tf.clip_by_value(distance_mat, -max_relative_position,\n",
        "                                          max_relative_position)\n",
        "  # Shift values to be >= 0. Each integer still uniquely identifies a relative\n",
        "  # position difference.\n",
        "  final_mat = distance_mat_clipped + max_relative_position\n",
        "  return final_mat\n",
        "\n",
        "@tf.function\n",
        "def _generate_relative_positions_embeddings(length,\n",
        "                                            max_relative_position,\n",
        "                                            embeddings_table):\n",
        "  \"\"\"Generates tensor of size [1 if cache else length, length, depth].\"\"\"\n",
        "  relative_positions_matrix = _generate_relative_positions_matrix(\n",
        "      length, max_relative_position)\n",
        "  #vocab_size = max_relative_position * 2 + 1\n",
        "  # Generates embedding for each relative position of dimension depth.\n",
        "  #embeddings_table = tf.get_variable(\"embeddings\", [vocab_size, depth])\n",
        "  embeddings = tf.gather(embeddings_table, relative_positions_matrix)\n",
        "  return embeddings\n",
        "\n",
        "@tf.function\n",
        "def _relative_attention_inner(x, y, z, transpose):\n",
        "  \"\"\"Relative position-aware dot-product attention inner calculation.\n",
        "  This batches matrix multiply calculations to avoid unnecessary broadcasting.\n",
        "  Args:\n",
        "    x: Tensor with shape [batch_size, heads, length, length or depth].\n",
        "    y: Tensor with shape [batch_size, heads, length, depth].\n",
        "    z: Tensor with shape [length, length, depth].\n",
        "    transpose: Whether to transpose inner matrices of y and z. Should be true if\n",
        "        last dimension of x is depth, not length.\n",
        "  Returns:\n",
        "    A Tensor with shape [batch_size, heads, length, length or depth].\n",
        "  \"\"\"\n",
        "  input_shape = K.shape(x)\n",
        "  batch_size, heads, length = input_shape[0], input_shape[1], input_shape[2]\n",
        "\n",
        "  # xy_matmul is [batch_size, heads, length, length or depth]\n",
        "  xy_matmul = tf.matmul(x, y, transpose_b=transpose)\n",
        "  # x_t is [length, batch_size, heads, length or depth]\n",
        "  x_t = tf.transpose(x, [2, 0, 1, 3])\n",
        "  # x_t_r is [length, batch_size * heads, length or depth]\n",
        "  x_t_r = tf.reshape(x_t, [length, heads * batch_size, -1])\n",
        "  # x_tz_matmul is [length batch_size * heads, length or depth]\n",
        "  x_tz_matmul = tf.matmul(x_t_r, z, transpose_b=transpose)\n",
        "  # x_tz_matmul_r is [length or 1, batch_size, heads, length or depth]\n",
        "  x_tz_matmul_r = tf.reshape(x_tz_matmul, [length, batch_size, heads, -1])\n",
        "  # x_tz_matmul_r_t is [batch_size, heads, length, length or depth]\n",
        "  x_tz_matmul_r_t = tf.transpose(x_tz_matmul_r, [1, 2, 0, 3])\n",
        "  return xy_matmul + x_tz_matmul_r_t\n",
        "\n",
        "#_generate_relative_positions_matrix(WINDOW_SIZE, 5)\n",
        "#etable = tf.Variable(initial_value=[np.arange(2*N_FEATURE+1)*0.1]*5, name=\"foo\")\n",
        "#_generate_relative_positions_embeddings(WINDOW_SIZE, 5, tf.transpose(etable))[:, :, 1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbTzmDHsd9Yx",
        "colab_type": "text"
      },
      "source": [
        "Transformer\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "cellView": "both",
        "id": "-qFPCP4kjrOt",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "import math\n",
        "\n",
        "@tf.function\n",
        "def gelu(x):\n",
        "  \"\"\"An approximation of gelu.\n",
        "  See: https://arxiv.org/pdf/1606.08415.pdf\n",
        "  \"\"\"\n",
        "  return 0.5 * x * (1.0 + K.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * K.pow(x, 3))))\n",
        "\n",
        "class LayerNormalization(keras.layers.Layer):\n",
        "  def __init__(self,\n",
        "                center=True,\n",
        "                scale=True,\n",
        "                epsilon=None,\n",
        "                gamma_initializer='ones',\n",
        "                beta_initializer='zeros',\n",
        "                gamma_regularizer=None,\n",
        "                beta_regularizer=None,\n",
        "                gamma_constraint=None,\n",
        "                beta_constraint=None,\n",
        "                **kwargs):\n",
        "      \"\"\"Layer normalization layer\n",
        "      See: [Layer Normalization](https://arxiv.org/pdf/1607.06450.pdf)\n",
        "      :param center: Add an offset parameter if it is True.\n",
        "      :param scale: Add a scale parameter if it is True.\n",
        "      :param epsilon: Epsilon for calculating variance.\n",
        "      :param gamma_initializer: Initializer for the gamma weight.\n",
        "      :param beta_initializer: Initializer for the beta weight.\n",
        "      :param gamma_regularizer: Optional regularizer for the gamma weight.\n",
        "      :param beta_regularizer: Optional regularizer for the beta weight.\n",
        "      :param gamma_constraint: Optional constraint for the gamma weight.\n",
        "      :param beta_constraint: Optional constraint for the beta weight.\n",
        "      :param kwargs:\n",
        "      \"\"\"\n",
        "      super(LayerNormalization, self).__init__(**kwargs)\n",
        "      self.supports_masking = False\n",
        "      self.center = center\n",
        "      self.scale = scale\n",
        "      if epsilon is None:\n",
        "          epsilon = K.epsilon() * K.epsilon()\n",
        "      self.epsilon = epsilon\n",
        "      self.gamma_initializer = keras.initializers.get(gamma_initializer)\n",
        "      self.beta_initializer = keras.initializers.get(beta_initializer)\n",
        "      self.gamma_regularizer = keras.regularizers.get(gamma_regularizer)\n",
        "      self.beta_regularizer = keras.regularizers.get(beta_regularizer)\n",
        "      self.gamma_constraint = keras.constraints.get(gamma_constraint)\n",
        "      self.beta_constraint = keras.constraints.get(beta_constraint)\n",
        "      self.gamma, self.beta = None, None\n",
        "\n",
        "  def get_config(self):\n",
        "      config = {\n",
        "          'center': self.center,\n",
        "          'scale': self.scale,\n",
        "          'epsilon': self.epsilon,\n",
        "          'gamma_initializer': keras.initializers.serialize(self.gamma_initializer),\n",
        "          'beta_initializer': keras.initializers.serialize(self.beta_initializer),\n",
        "          'gamma_regularizer': keras.regularizers.serialize(self.gamma_regularizer),\n",
        "          'beta_regularizer': keras.regularizers.serialize(self.beta_regularizer),\n",
        "          'gamma_constraint': keras.constraints.serialize(self.gamma_constraint),\n",
        "          'beta_constraint': keras.constraints.serialize(self.beta_constraint),\n",
        "      }\n",
        "      base_config = super(LayerNormalization, self).get_config()\n",
        "      return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "  def compute_output_shape(self, input_shape):\n",
        "      return input_shape\n",
        "\n",
        "  def compute_mask(self, inputs, input_mask=None):\n",
        "      return input_mask\n",
        "\n",
        "  def build(self, input_shape):\n",
        "      self.input_spec = keras.layers.InputSpec(shape=input_shape)\n",
        "      shape = input_shape[-1:]\n",
        "      if self.scale:\n",
        "          self.gamma = self.add_weight(\n",
        "              shape=shape,\n",
        "              initializer=self.gamma_initializer,\n",
        "              regularizer=self.gamma_regularizer,\n",
        "              constraint=self.gamma_constraint,\n",
        "              name='gamma',\n",
        "          )\n",
        "      if self.center:\n",
        "          self.beta = self.add_weight(\n",
        "              shape=shape,\n",
        "              initializer=self.beta_initializer,\n",
        "              regularizer=self.beta_regularizer,\n",
        "              constraint=self.beta_constraint,\n",
        "              name='beta',\n",
        "          )\n",
        "      super(LayerNormalization, self).build(input_shape)\n",
        "\n",
        "  def call(self, inputs, training=None):\n",
        "    mean = K.mean(inputs, axis=-1, keepdims=True)\n",
        "    variance = K.mean(K.square(inputs - mean), axis=-1, keepdims=True)\n",
        "    std = K.sqrt(variance + self.epsilon)\n",
        "    outputs = (inputs - mean) / std\n",
        "    if self.scale:\n",
        "        outputs *= self.gamma\n",
        "    if self.center:\n",
        "        outputs += self.beta\n",
        "\n",
        "    return outputs\n",
        "\n",
        "class MultiHeadAttentionNoWeight(keras.layers.Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    self.activation = gelu\n",
        "    self.supports_masking = False\n",
        "    super(MultiHeadAttentionNoWeight, self).__init__(**kwargs)\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    super(MultiHeadAttentionNoWeight, self).build(input_shape)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    input_shape = K.shape(inputs)\n",
        "    #batch_size, seq_len, head_num, head_dim = inputs[0], inputs[1], inputs[2], inputs[3]\n",
        "    #query = K.reshape(inputs, (batch_size * seq_len, head_num, head_dim))\n",
        "    #key = K.reshape(inputs, (batch_size * seq_len, head_num, head_dim))\n",
        "    #value = K.reshape(inputs, (batch_size * seq_len, head_num, head_dim))\n",
        "    query = key = value = inputs\n",
        "    e = K.batch_dot(query, key, axes=3) / K.sqrt(K.cast(N_FEATURE, dtype=K.floatx()))\n",
        "    e = K.exp(e - K.max(e, axis=-1, keepdims=True))\n",
        "    a = e / (K.sum(e, axis=-1, keepdims=True) + K.epsilon())\n",
        "    y = K.batch_dot(a, value)\n",
        "    \n",
        "    y = self.activation(y)\n",
        "\n",
        "    return y\n",
        "  \n",
        "class MultiHeadAttention(keras.layers.Layer):\n",
        "    \"\"\"Multi-head attention layer.\n",
        "    See: https://arxiv.org/pdf/1706.03762.pdf\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 head_num,\n",
        "                 max_relative_position=5,\n",
        "                 use_bias=True,\n",
        "                 kernel_initializer='glorot_normal',\n",
        "                 bias_initializer='zeros',\n",
        "                 kernel_regularizer=None,\n",
        "                 bias_regularizer=None,\n",
        "                 kernel_constraint=None,\n",
        "                 bias_constraint=None,\n",
        "                 history_only=False,\n",
        "                 **kwargs):\n",
        "        \"\"\"Initialize the layer.\n",
        "        :param head_num: Number of heads.\n",
        "        :param use_bias: Whether to use bias term.\n",
        "        :param kernel_initializer: Initializer for linear mappings.\n",
        "        :param bias_initializer: Initializer for linear mappings.\n",
        "        :param kernel_regularizer: Regularizer for linear mappings.\n",
        "        :param bias_regularizer: Regularizer for linear mappings.\n",
        "        :param kernel_constraint: Constraints for linear mappings.\n",
        "        :param bias_constraint: Constraints for linear mappings.\n",
        "        :param history_only: Whether to only use history in attention layer.\n",
        "        \"\"\"\n",
        "        self.supports_masking = False\n",
        "        self.head_num = head_num\n",
        "        self.max_relative_position = max_relative_position\n",
        "        self.activation = gelu\n",
        "        self.use_bias = use_bias\n",
        "        self.kernel_initializer = keras.initializers.get(kernel_initializer)\n",
        "        self.bias_initializer = keras.initializers.get(bias_initializer)\n",
        "        self.kernel_regularizer = keras.regularizers.get(kernel_regularizer)\n",
        "        self.bias_regularizer = keras.regularizers.get(bias_regularizer)\n",
        "        self.kernel_constraint = keras.constraints.get(kernel_constraint)\n",
        "        self.bias_constraint = keras.constraints.get(bias_constraint)\n",
        "        self.history_only = history_only\n",
        "\n",
        "        self.Wq, self.Wk, self.Wv, self.Wo = None, None, None, None\n",
        "        self.bq, self.bk, self.bv, self.bo = None, None, None, None\n",
        "        super(MultiHeadAttention, self).__init__(**kwargs)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            'head_num': self.head_num,\n",
        "            'max_relative_position': self.max_relative_position,\n",
        "            'use_bias': self.use_bias,\n",
        "            'kernel_initializer': keras.initializers.serialize(self.kernel_initializer),\n",
        "            'bias_initializer': keras.initializers.serialize(self.bias_initializer),\n",
        "            'kernel_regularizer': keras.regularizers.serialize(self.kernel_regularizer),\n",
        "            'bias_regularizer': keras.regularizers.serialize(self.bias_regularizer),\n",
        "            'kernel_constraint': keras.constraints.serialize(self.kernel_constraint),\n",
        "            'bias_constraint': keras.constraints.serialize(self.bias_constraint),\n",
        "            'history_only': self.history_only,\n",
        "        }\n",
        "        base_config = super(MultiHeadAttention, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        if isinstance(input_shape, list):\n",
        "            q, k, v = input_shape\n",
        "            return q[:-1] + (v[-1],)\n",
        "        return input_shape\n",
        "\n",
        "    def compute_mask(self, inputs, input_mask=None):\n",
        "        if isinstance(input_mask, list):\n",
        "            return input_mask[0]\n",
        "        return input_mask\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        new_feature_dim = WINDOW_SIZE * N_FEATURE\n",
        "        self.Wq = self.add_weight(\n",
        "            shape=(new_feature_dim, new_feature_dim),\n",
        "            initializer=self.kernel_initializer,\n",
        "            regularizer=self.kernel_regularizer,\n",
        "            constraint=self.kernel_constraint,\n",
        "            name='%s_Wq' % self.name,\n",
        "        )\n",
        "        if self.use_bias:\n",
        "            self.bq = self.add_weight(\n",
        "                shape=(new_feature_dim,),\n",
        "                initializer=self.bias_initializer,\n",
        "                regularizer=self.bias_regularizer,\n",
        "                constraint=self.bias_constraint,\n",
        "                name='%s_bq' % self.name,\n",
        "            )\n",
        "        self.Wk = self.add_weight(\n",
        "            shape=(new_feature_dim, new_feature_dim),\n",
        "            initializer=self.kernel_initializer,\n",
        "            regularizer=self.kernel_regularizer,\n",
        "            constraint=self.kernel_constraint,\n",
        "            name='%s_Wk' % self.name,\n",
        "        )\n",
        "        if self.use_bias:\n",
        "            self.bk = self.add_weight(\n",
        "                shape=(new_feature_dim,),\n",
        "                initializer=self.bias_initializer,\n",
        "                regularizer=self.bias_regularizer,\n",
        "                constraint=self.bias_constraint,\n",
        "                name='%s_bk' % self.name,\n",
        "            )\n",
        "        self.Wv = self.add_weight(\n",
        "            shape=(new_feature_dim, new_feature_dim),\n",
        "            initializer=self.kernel_initializer,\n",
        "            regularizer=self.kernel_regularizer,\n",
        "            constraint=self.kernel_constraint,\n",
        "            name='%s_Wv' % self.name,\n",
        "        )\n",
        "        if self.use_bias:\n",
        "            self.bv = self.add_weight(\n",
        "                shape=(new_feature_dim,),\n",
        "                initializer=self.bias_initializer,\n",
        "                regularizer=self.bias_regularizer,\n",
        "                constraint=self.bias_constraint,\n",
        "                name='%s_bv' % self.name,\n",
        "            )\n",
        "        self.Wo = self.add_weight(\n",
        "            shape=(new_feature_dim, new_feature_dim),\n",
        "            initializer=self.kernel_initializer,\n",
        "            regularizer=self.kernel_regularizer,\n",
        "            constraint=self.kernel_constraint,\n",
        "            name='%s_Wo' % self.name,\n",
        "        )\n",
        "        if self.use_bias:\n",
        "            self.bo = self.add_weight(\n",
        "                shape=(new_feature_dim,),\n",
        "                initializer=self.bias_initializer,\n",
        "                regularizer=self.bias_regularizer,\n",
        "                constraint=self.bias_constraint,\n",
        "                name='%s_bo' % self.name,\n",
        "            )\n",
        "        self.position_emb_table = self.add_weight(\n",
        "            shape=(2*self.max_relative_position+1, N_FEATURE),\n",
        "            initializer=self.kernel_initializer,\n",
        "            regularizer=self.kernel_regularizer,\n",
        "            constraint=self.kernel_constraint,\n",
        "            name='%s_PositionEmbTable' % self.name,            \n",
        "        )\n",
        "        super(MultiHeadAttention, self).build(input_shape)\n",
        "\n",
        "    @staticmethod\n",
        "    def _reshape_to_batches(x, head_num):\n",
        "        input_shape = K.shape(x)\n",
        "        batch_size, seq_len, feature_dim = input_shape[0], input_shape[1], input_shape[2]\n",
        "        head_dim = feature_dim // head_num\n",
        "        x = K.reshape(x, (batch_size, seq_len, head_num, head_dim))\n",
        "        x = K.permute_dimensions(x, [0, 2, 1, 3])\n",
        "        return x #K.reshape(x, (batch_size * head_num, seq_len * head_dim))\n",
        "\n",
        "    @staticmethod\n",
        "    def _reshape_from_batches(x, head_num):\n",
        "        input_shape = K.shape(x)\n",
        "        batch_size, seq_len, feature_dim = input_shape[0], input_shape[1], input_shape[2]\n",
        "        x = K.reshape(x, (batch_size // head_num, head_num, seq_len, feature_dim))\n",
        "        x = K.permute_dimensions(x, [0, 2, 1, 3])\n",
        "        return K.reshape(x, (batch_size // head_num, seq_len, feature_dim * head_num))\n",
        "\n",
        "    @staticmethod\n",
        "    def _reshape_mask(mask, head_num):\n",
        "        if mask is None:\n",
        "            return mask\n",
        "        seq_len = K.shape(mask)[1]\n",
        "        mask = K.expand_dims(mask, axis=1)\n",
        "        mask = K.tile(mask, [1, head_num, 1])\n",
        "        return K.reshape(mask, (-1, seq_len))\n",
        "\n",
        "    def call(self, inputs):\n",
        "        batch_size = K.shape(inputs)[0]\n",
        "        x = K.permute_dimensions(inputs, [0, 2, 1, 3])\n",
        "        x = K.reshape(x, (batch_size, N_STOCK, WINDOW_SIZE * N_FEATURE))\n",
        "        q = k = v = x\n",
        "        q = K.dot(q, self.Wq)\n",
        "        k = K.dot(k, self.Wk)\n",
        "        v = K.dot(v, self.Wv)\n",
        "        if self.use_bias:\n",
        "            q += self.bq\n",
        "            k += self.bk\n",
        "            v += self.bv\n",
        "        if self.activation is not None:\n",
        "            q = self.activation(q)\n",
        "            k = self.activation(k)\n",
        "            v = self.activation(v)\n",
        "\n",
        "        position_emb = _generate_relative_positions_embeddings(\n",
        "            WINDOW_SIZE, self.max_relative_position, self.position_emb_table)\n",
        "        query = K.reshape(q, (batch_size, N_STOCK, WINDOW_SIZE, N_FEATURE))\n",
        "        key = K.reshape(k, (batch_size, N_STOCK, WINDOW_SIZE, N_FEATURE))\n",
        "        value = K.reshape(v, (batch_size, N_STOCK, WINDOW_SIZE, N_FEATURE))\n",
        "        \n",
        "        e = _relative_attention_inner(query, key, position_emb, True)\n",
        "        e = K.exp(e - K.max(e, axis=-1, keepdims=True)) # [B, H, L, L]\n",
        "        a = e / (K.sum(e, axis=-1, keepdims=True) + K.epsilon())\n",
        "        y = K.batch_dot(a, value)\n",
        "        y = K.reshape(y, (batch_size, N_STOCK, WINDOW_SIZE * N_FEATURE))\n",
        "        \n",
        "        y = K.dot(y, self.Wo)\n",
        "        if self.use_bias:\n",
        "            y += self.bo\n",
        "        if self.activation is not None:\n",
        "            y = self.activation(y)\n",
        "            \n",
        "        y = K.reshape(y, (batch_size, N_STOCK, WINDOW_SIZE, N_FEATURE))\n",
        "        y = K.permute_dimensions(y, [0, 2, 1, 3])\n",
        "        return y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajGvSaVMeCEt",
        "colab_type": "text"
      },
      "source": [
        "Full model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIJuE49OcSwg",
        "colab_type": "code",
        "outputId": "3c4a4394-6d3d-4d1a-9cd3-f00602b9b3e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "class TsTransformer(keras.Model):\n",
        "    def __init__(self):\n",
        "        super(TsTransformer, self).__init__()\n",
        "\n",
        "#        self.attn_xs = MultiHeadAttentionNoWeight()\n",
        "        # self.attn_ts = MultiHeadAttention(head_num=N_STOCK, max_relative_position=5)\n",
        "        # self.dropout = keras.layers.Dropout(rate=0.3)\n",
        "        # self.layer_norm = LayerNormalization()\n",
        "        self.fc = keras.layers.Dense(N_STOCK, input_shape=(N_STOCK*N_FEATURE,))\n",
        "\n",
        "    def call(self, inputs, training=None, mask=None):\n",
        "      # inputs is expected to have the shape (batch_size, WINDOW_SIZE, N_STOCK, N_FEATURE)\n",
        "      x = inputs\n",
        "      input_shape = K.shape(x)\n",
        "      batch_size = input_shape[0]\n",
        "      for i in range(2):\n",
        "        input_x = x\n",
        " #       x = self.attn_xs(x)\n",
        "        # x = self.attn_ts(x)\n",
        "        # x = self.fc(x)\n",
        "        # x = self.dropout(x)\n",
        "        # x = input_x + x\n",
        "        # x = self.layer_norm(x)\n",
        "        # input_x = x\n",
        "        # x = self.attn_ts(x)\n",
        "        # x = self.fc(x)\n",
        "        # x = self.dropout(x)\n",
        "        # x = input_x + x\n",
        "        # x = self.layer_norm(x)\n",
        "      x = self.fc(K.reshape(x[:, -1, :], (batch_size, N_STOCK * N_FEATURE)))\n",
        "\n",
        "      return x\n",
        "\n",
        "model = TsTransformer()\n",
        "model.build(input_shape=(None, WINDOW_SIZE, N_STOCK, N_FEATURE))\n",
        "model.summary()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"ts_transformer_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              multiple                  38808     \n",
            "=================================================================\n",
            "Total params: 38,808\n",
            "Trainable params: 38,808\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4mLgqqqoU9l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tfv1.reset_default_graph()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VK0VkJyM9448",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#logs_base_dir = \"./logs\"\n",
        "#os.makedirs(logs_base_dir, exist_ok=True)\n",
        "#%load_ext tensorboard\n",
        "#%tensorboard --logdir {logs_base_dir}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LGYkRQcen62",
        "colab_type": "code",
        "outputId": "ea2cc64e-e239-49bd-8625-ae9796551c5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "batch_size = 32\n",
        "epochs = 3\n",
        "\n",
        "@tf.function\n",
        "def compute_loss(y_pred, y):\n",
        "  y = tf.cast(y, tf.float32)\n",
        "  return -tf.reduce_mean(tfp.stats.correlation(y_pred, y, sample_axis=1, event_axis=None) * tfp.stats.stddev(y, sample_axis=1))\n",
        "\n",
        "optimizer = keras.optimizers.Adam(0.001)\n",
        "\n",
        "if False:\n",
        "  model.compile(optimizer=optimizer,\n",
        "                loss=keras.losses.MeanSquaredError())\n",
        "\n",
        "  logdir = os.path.join(logs_base_dir, dt.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "  tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
        "\n",
        "  # train\n",
        "  model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs,\n",
        "            validation_data=(x_test, y_test), callbacks=[tensorboard_callback],\n",
        "            verbose=1)\n",
        "\n",
        "  # evaluate on test set\n",
        "  scores = model.evaluate(x_test, y_test, batch_size, verbose=1)\n",
        "  print(\"Final test loss and accuracy :\", scores)\n",
        "else:\n",
        "  count = 0; avg_loss = None; loss_buffer = []; stop_flag = False\n",
        "  for epoch in range(epochs):\n",
        "    ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "    ds = ds.shuffle(x_train.shape[0]).batch(batch_size)\n",
        "    for step, (xs, ys) in enumerate(ds):\n",
        "      with tf.GradientTape() as tape:\n",
        "        y_pred = model(tf.cast(xs, tf.float32))\n",
        "        loss = compute_loss(y_pred, ys)\n",
        "\n",
        "      # compute gradient\n",
        "      grads = tape.gradient(loss, model.trainable_variables)\n",
        "      grads = [tf.where(tf.math.is_nan(g), tf.zeros_like(g), g) for g in grads]\n",
        "      # update to weights\n",
        "      optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "      val_loss = float(compute_loss(model(tf.cast(x_val, tf.float32)), y_val))*1e4\n",
        "      print(epoch, step, float(loss)*1e4, val_loss)\n",
        "      count = count + 1; loss_buffer.append(val_loss)\n",
        "      if count % 5 == 0:\n",
        "        new_avg_loss = np.mean(np.array(loss_buffer))\n",
        "        #if avg_loss is not None and new_avg_loss > avg_loss:\n",
        "        #  stop_flag = True\n",
        "        #  break\n",
        "        loss_buffer = []\n",
        "        avg_loss = new_avg_loss\n",
        "    if stop_flag:\n",
        "      break"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0 2.330923598492518 2.174522669520229\n",
            "0 1 -0.5660254100803286 2.134860696969554\n",
            "0 2 -1.9029091345146298 2.090511698042974\n",
            "0 3 -1.6445523942820728 2.0375847816467285\n",
            "0 4 -2.025749417953193 1.9710765627678484\n",
            "0 5 -3.653267049230635 1.9165920093655586\n",
            "0 6 -2.3035830236040056 1.8662717775441706\n",
            "0 7 2.835286722984165 1.8392714264336973\n",
            "0 8 3.198060439899564 1.823230559239164\n",
            "0 9 1.6192867769859731 1.8092765822075307\n",
            "0 10 -0.9400833368999884 1.8039523274637759\n",
            "0 11 -2.1051321527920663 1.7962830315809697\n",
            "0 12 1.490817521698773 1.7822244262788445\n",
            "0 13 2.2022398479748517 1.762453030096367\n",
            "0 14 0.00398969859816134 1.749138900777325\n",
            "0 15 -1.0807072976604104 1.731012889649719\n",
            "0 16 0.07396716682706028 1.7088261665776372\n",
            "0 17 -1.1802566586993635 1.6826855426188558\n",
            "0 18 1.3026021770201623 1.6369210788980126\n",
            "0 19 0.039498991100117564 1.5939236618578434\n",
            "0 20 -2.455766953062266 1.5597291348967701\n",
            "0 21 -1.6023848729673773 1.5349630848504603\n",
            "0 22 3.3307509147562087 1.5225289098452777\n",
            "0 23 -0.5701422924175858 1.536940544610843\n",
            "1 0 -3.2588315661996603 1.5482922026421875\n",
            "1 1 -7.348292274400592 1.538961660116911\n",
            "1 2 -4.348184447735548 1.5309028094634414\n",
            "1 3 -4.956243792548776 1.5232854639180005\n",
            "1 4 -7.1780342841520905 1.51461674249731\n",
            "1 5 -0.6086136636440642 1.5001380234025419\n",
            "1 6 -1.1326964158797637 1.493832969572395\n",
            "1 7 -4.489401471801102 1.4907841978129\n",
            "1 8 -6.29185582511127 1.5014679229352623\n",
            "1 9 -9.58506134338677 1.5019792772363871\n",
            "1 10 -5.479578976519406 1.498571946285665\n",
            "1 11 -3.0506961047649384 1.4911440666764975\n",
            "1 12 -4.932369920425117 1.4862148964311928\n",
            "1 13 -4.566212301142514 1.4810207358095795\n",
            "1 14 -3.7379050627350807 1.4731193368788809\n",
            "1 15 -4.353780823294073 1.4569357153959572\n",
            "1 16 -4.788805963471532 1.4368013944476843\n",
            "1 17 -3.276378265582025 1.430385309504345\n",
            "1 18 -3.266621788498014 1.4258363808039576\n",
            "1 19 -5.440848181024194 1.4231276873033494\n",
            "1 20 -4.1080176015384495 1.422771019861102\n",
            "1 21 -5.852815229445696 1.4133905642665923\n",
            "1 22 -9.35858057346195 1.4001474482938647\n",
            "1 23 -6.425001192837954 1.4208906213752925\n",
            "2 0 -1.8918453133665025 1.4388372073881328\n",
            "2 1 -7.617502706125379 1.4635226398240775\n",
            "2 2 -9.707501158118248 1.4830840518698096\n",
            "2 3 -7.457301253452897 1.5013401571195573\n",
            "2 4 -11.716994922608137 1.5168491518124938\n",
            "2 5 -11.231867829337716 1.521484082331881\n",
            "2 6 -8.840957889333367 1.520719233667478\n",
            "2 7 -7.961940718814731 1.51468746480532\n",
            "2 8 -9.658625349402428 1.51059080963023\n",
            "2 9 -13.142833486199379 1.510550209786743\n",
            "2 10 -13.7021963018924 1.5200575580820441\n",
            "2 11 -9.67437750659883 1.519310026196763\n",
            "2 12 -7.420169422402978 1.516849297331646\n",
            "2 13 -7.213137578219175 1.4965525770094246\n",
            "2 14 -7.4474012944847345 1.4847672719042748\n",
            "2 15 -6.71862275339663 1.4757145254407078\n",
            "2 16 -2.7765269624069333 1.4738363097421825\n",
            "2 17 -7.838754099793732 1.4751244452781975\n",
            "2 18 -10.884860530495644 1.474416785640642\n",
            "2 19 -4.223005962558091 1.4755902520846575\n",
            "2 20 -10.222753044217825 1.4699705934617668\n",
            "2 21 -8.353334269486368 1.457579928683117\n",
            "2 22 -3.5549880703911185 1.4384348469320685\n",
            "2 23 -8.69966926984489 1.431232230970636\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XwNyd6Br58V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ea96ed45-3733-4c4e-ec73-bb23b3395c68"
      },
      "source": [
        ""
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(740, 20, 88, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xNkvzuNftU5",
        "colab_type": "code",
        "outputId": "bbbfd421-ce20-4117-d1cb-661855ce5dd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "-compute_loss(model(x_test), tf.cast(y_test, tf.float32))*1e4"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer ts_transformer_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=0.737475>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tw05DLkfS-7S",
        "colab_type": "code",
        "outputId": "5247d255-e946-4bed-9816-aa0f2e64231b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "import statsmodels.api as sm\n",
        "\n",
        "y_hat = model(x_test)\n",
        "X = np.expand_dims(np.array(y_hat).flatten(), axis=-1)\n",
        "y = np.array(y_test).flatten()\n",
        "reg = LinearRegression(fit_intercept=True).fit(X, y)\n",
        "np.sqrt(reg.score(X, y))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer ts_transformer_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.004270088424607299"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxYELtaaik9Q",
        "colab_type": "code",
        "outputId": "e96dec76-0250-40e7-92af-a488f3d0ecaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.sqrt(reg.score(X, y)) * np.std(y) * 1e4"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5556694111285609"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-3dm69gj9cN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}